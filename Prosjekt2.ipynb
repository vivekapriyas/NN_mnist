{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>\n",
    "    <center>\n",
    "        TMA4320 Prosjekt 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Innholdsfortegnelse\n",
    "\n",
    "### 1. Introduksjon\n",
    "### 2. Algoritme og hjelpefunksjoner\n",
    "### 3. Første testproblem: Punkter i planet\n",
    "> ### 3.1 PIP: Plain vanilla gradient descent\n",
    "> ### 3.2 PIP: ADAM descent\n",
    "### 4. Andre testproblem: MNIST - siffergjenkjenning\n",
    "### 5. Avslutning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduksjon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Formålet med prosjektet er å gi en introduksjon til maskinlæring, nærmere bestemt dyplæring og dype nevrale nettverk, ved egen implementering av algoritmene. \n",
    "Ideen er å trene dataprogrammet til å gjenkjenne bildeelementer i to ulike testproblemer. I det første testproblemet er problemstillingen forenklet til gjenkjenning av fargede datapunkt, mens i det andre testproblemet skal programmet kunne skille mellom to ulike håndskrevne sifre. \n",
    "Denne rapporten skal først presentere den grunneleggende algoritmen og dens hjelpefunksjoner som benyttes i treningsfasen og hovedsaklig er lik for begge problemstillingene, før hver problemstilling blir framstilt i detalj.\n",
    "Valg av startparametre skal begrunnes og parametrene som ble funnet i treningsfasen skal presenteres. Deretter skal modellen med disse fikserte parametrene kjøres med nye data og resultatene bli lagt fram og diskutert."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Algoritme og hjelpefunksjoner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import math\n",
    "import random "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from loader import load_mnist\n",
    "from loader import get_dataset\n",
    "from spirals import get_data_spiral_2d\n",
    "from plotting import plot_progression, plot_model, plot_separation\n",
    "from files import write_spiral_to_file, read_spiral_from_file, writeParams, readParams\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importerte funksjoner fra filer som er brukt i prosjektet. Disse er brukt for å generere data for det første testproblemet, lese inn data for bildene i MNIST for det andre testproblemet og for å plotte resultatet.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algoritmen er blitt delt inn i flere mindre funksjoner for å forenkle kodestrukturen og gjøre den mer oversiktlig. \n",
    "Hovedsaklig er den delt inn i å transformere input-data mellom lagene, å bestemme gradienten til $J = \\frac{1}{2}||\\bf{Z-c}||$ med hensyn på vekter, bias, $\\omega$ og $\\mu$ og optimering av nettverket med hensyn på disse. I tillegg til dette kommer flere hjelpefunksjoner som benyttes av de overordnede funksjonene."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getRandomParam(d=2, K=20):\n",
    "    w_k=np.random.randn(K,d,d)\n",
    "    b_k=np.random.randn(K,d)\n",
    "    omega = np.random.randn(d)\n",
    "    my = np.random.rand(1)\n",
    "    return w_k,b_k,omega, my"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Som sett ovenfor genererer getRandomParam tilfeldige startverdier for vekter, bias, $\\omega$ og $\\mu$ ved å trekke fra en standardnormalfordeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigma(x):\n",
    "    return(np.tanh(x))\n",
    "\n",
    "def sigma_diff(x):\n",
    "    return(1-np.tanh(x)**2)\n",
    "\n",
    "def eta(x):\n",
    "    x[x > 350] = 350\n",
    "    return(np.exp(x)/(np.exp(x)+1))\n",
    "\n",
    "def eta_diff(x):\n",
    "    x[x > 350] = 350\n",
    "    return np.exp(x)/(np.exp(x) + 1)**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aktiveringsfunksjonen $\\sigma$ ovenfor benyttes i transformasjonen av $\\bf{Y_k}$ mellom lagene, mens $\\eta$ brukes i projiseringen fra siste lag på en skalar z. \n",
    "Både $\\sigma'$ og $\\eta'$ benyytes i utledningen av gradienten.\n",
    "\n",
    "### CARL FREDRIK: BEGRUNN CAP-INGEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  def findNextYVec(Y_k, W_k, b_k, h):\n",
    "    vec_for_sigma = np.dot(W_k, Y_k) + b_k\n",
    "    sigma_vec = sigma(vec_for_sigma)\n",
    "    return(Y_k+h*sigma_vec)\n",
    "\n",
    "def findNextYMat(Y_k_mat, W_k, b_k, h):\n",
    "    numberOfColumns = Y_k_mat.shape[1]\n",
    "    nextYMat = np.zeros((Y_k_mat.shape[0], numberOfColumns))\n",
    "    for column in range(numberOfColumns):\n",
    "        Y_k = Y_k_mat[:, column]\n",
    "        nextYMat[:, column] = findNextYVec(Y_k, W_k, b_k, h)\n",
    "    return(np.array(nextYMat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CARL FREDRIK: KOMMENTER DETTE ^^"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findZvalue(Y_k_mat, w_vec, my):\n",
    "    vec_for_eta = np.dot(w_vec, Y_k_mat) + my\n",
    "    return(eta(vec_for_eta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def J(Z,c):\n",
    "    return 0.5*np.linalg.norm(Z-c)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def makeMatrixFromVec(a, b):\n",
    "    return_matrix = np.zeros((len(a), len(b)))\n",
    "    for i in range(len(b)):\n",
    "        return_matrix[:, i] = b[i]*a\n",
    "    return(return_matrix)\n",
    "\n",
    "def findGradJ(Z_vec, W_K, Y_K, Y_end, w_vec, c_vec, my, b_K, h, K):\n",
    "    #Regner ut et produkt som går igjen for å spare noen utregninger\n",
    "    prod_to_re_use = eta_diff(np.dot(w_vec, Y_end)+my) \n",
    "    J_diff_my = np.dot(prod_to_re_use, Z_vec-c_vec)\n",
    "    #dJ/dw  = Y_K * [(Z-c)had*eta_diff(Y_K^T*w + my*1Vec]\n",
    "    #had* er hadamard-produktet, som bare er å gange sammen alle elementene for seg, 1Vec er en vektor fylt med enere\n",
    "    diff_my_vec_for_product = np.multiply(Z_vec - c_vec, prod_to_re_use)\n",
    "    J_diff_w = np.dot(Y_end, diff_my_vec_for_product)\n",
    "    #Definerer P_K = dJ/dY_K\n",
    "    #dJ/dY_K = w_vec * [(Z_vec - c_vec)had*eta_diff(Y_K^T * w_vec + my*Vec1)]^T\n",
    "    #Rekursivt:\n",
    "    #P_k-1: P_k + h*W_k-1^T*[sigma_diff(W_k-1*Y_k-1 + b_k-1)had*P_k)\n",
    "    P_K = makeMatrixFromVec(w_vec, np.multiply(Z_vec - c_vec, prod_to_re_use))\n",
    "    #Ønsker nå å samle opp for alle de deriverte k < K:\n",
    "    P_k_liste = [P_K]\n",
    "    #Ønsker å lage b_k som en matrise, b_k har samme dimensjon som en kolonne av WdotY, men den må være like lang\n",
    "    for i in range(K-1, -1, -1):\n",
    "        #Starter å indeksere fra toppen, fordi W_k og Y_k inneholder k < K\n",
    "        WdotY = np.dot(W_K[i], Y_K[i])\n",
    "        b_i_mat = np.array([np.array([b_K[i][j] for e in range(WdotY.shape[1])]) for j in range(WdotY.shape[0])])\n",
    "        matrix_for_sigma_diff =  WdotY + b_i_mat\n",
    "        matrix_for_product = np.multiply(sigma_diff(matrix_for_sigma_diff), P_k_liste[0])\n",
    "        P_i = P_k_liste[0] + h * np.dot(np.transpose(W_K[i]), matrix_for_product)\n",
    "        P_k_liste.insert(0, P_i)\n",
    "    J_diff_W_matrix = []\n",
    "    #dJ/dW_k = h*[P_k+1 *had sigma_diff(W_k*Y_k + b_k)]*Y_k^T\n",
    "    J_diff_b_vec = []\n",
    "    #dJ/db_k = h*[P_k+1 *had sigma_diff(W_k*Y_k + b_k)]*\n",
    "    for j in range(K):\n",
    "        W_jY_j = np.dot(W_K[j], Y_K[j])\n",
    "        b_j_mat = np.array([np.array([b_K[j][c] for e in range(W_jY_j.shape[1])]) for c in range(W_jY_j.shape[0])])\n",
    "        one_vector = np.full((W_jY_j.shape[1]), 1)\n",
    "        J_diff_W_matrix.append(h*np.dot(P_k_liste[j+1]*sigma_diff(W_jY_j + b_j_mat), np.transpose(Y_K[j])))\n",
    "        J_diff_b_vec.append(h*np.dot(P_k_liste[j+1]*eta_diff(W_jY_j + b_j_mat), one_vector))\n",
    "    return(J_diff_W_matrix, J_diff_b_vec, J_diff_w, J_diff_my)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def newU(W_k,b_vec,w,my,tau,J_diff_W_matrix, J_diff_b_vec, J_diff_w, J_diff_my):\n",
    "    W_k = W_k - np.multiply(J_diff_W_matrix,tau)\n",
    "    b_vec = b_vec - np.multiply(J_diff_b_vec, tau)\n",
    "    w = w - tau*J_diff_w\n",
    "    my = my -tau*J_diff_my\n",
    "    return W_k, b_vec,w,my\n",
    "\n",
    "def adam2(m,v,g,j):\n",
    "    beta1 = 0.9\n",
    "    beta2 = 0.999\n",
    "    alpha = 0.01\n",
    "    epsilon = 1.0E-8\n",
    "\n",
    "    m = np.multiply(beta1,m) + np.multiply((1-beta1),g)\n",
    "    v = np.multiply(beta2,v) + np.multiply((1-beta2),np.multiply(g,g))\n",
    "    m_hat = np.multiply(m,1/(1-beta1**j))\n",
    "    v_hat = np.multiply(v,1/(1-beta2**j))\n",
    "    \n",
    "    J = np.multiply(alpha,np.multiply(m_hat,1/((np.sqrt(v_hat)+epsilon))))\n",
    "    \n",
    "    return m,v,J\n",
    "\n",
    "def getStartMandV(w,b,o,m):\n",
    "    mW = np.zeros_like(w)\n",
    "    mb = np.zeros_like(b)\n",
    "    mOmega = np.zeros_like(o)\n",
    "    mMy = np.zeros_like(m)\n",
    "\n",
    "    return mW,mb,mOmega,mMy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Funksjonene ovenfor benyttes for å optimere vekter, bias, $\\omega$ og $\\mu$.\n",
    "newU tilsvarer en plain vanilla gradient descent, der vi følger gradienten i avtagende retning og der læringsparameteren $\\tau$ bestemmer hvor langt vi følger gradienten.\n",
    "\n",
    "Funksjonen adam2 tilsvarer ADAM descent metoden, som tilpasser læringsraten til parameteren, dvs. vi beveger oss ulik lengde langsmed gradienten med hensyn på ulike parametre.\n",
    "getStartMandV er en hjelpefunksjon som genererer startparametrene m og v, som fungerer som et slags gjennomsnitt og kvadrert gjennomsnitt av tidligere iterasjoners gradienter, for adamfunksjonen. Før første iterasjon ønsker vi å sette m=v=0 for alle parametre."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def success(z, int_labels):\n",
    "    z = np.asanyarray(np.around(z, 0), dtype = int)\n",
    "    sum = 0\n",
    "    for i in range(len(z)):\n",
    "        if z[i] == int_labels[i]:\n",
    "            sum += 1\n",
    "    return(sum/len(z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Funksjonen success() sammenlikner $Z$ med label for hvert bilde og returnerer prosentandelen som stemmer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_function(Y_0, W_k, b_k, h):\n",
    "    Y_k = [Y_0]\n",
    "    for i in range(W_k.shape[0]):\n",
    "        Y_next = findNextYMat(Y_k[-1], W_k[i], b_k[i], h)\n",
    "        Y_k.append(Y_next)\n",
    "    return(Y_k)\n",
    "\n",
    "#Forward tar inn Y_0, og lager Y_k helt til K ved hjelp av de fire parameterne. Må ta inn de learned parameterne. \n",
    "#Forward function skal bruke parameterne som allerede er oppdaterte og lagret i minnet, men den skal bruke de på nye\n",
    "#datapunkter Y_0, og gå gjennom alle k helt til Y_K.\n",
    "\n",
    "#Last funtion tar inn den siste Y_K, og returnere Z for denne matrisen. \n",
    "    \n",
    "def last_function(Y_K_final, omega, my): #Tar inn Y_K, det siste laget. Altså Y_K final. Første paramter er grid\n",
    "    Z=findZvalue(Y_K_final, omega, my)\n",
    "    return Z  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Forward_function() og last_function() er hjelpefunksjoner for å plotte det første testproblemet. Forward_function() tar den første matrisen $Y_{0}$ og finner den neste matrisen $Y_{k+1}$ ved hjelp av de lærte parameterne. Dette gjør den for alle matriser $K$. Last_function() tar derimot inn kun den siste matrisen $Y_{K}$ og returnerer $Z$ for denne. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotJ(J_liste):\n",
    "    x_akse = np.arange(len(J_liste))\n",
    "    plt.scatter(x_akse, J_liste)\n",
    "    plt.xlabel(\"Iterasjoner\")\n",
    "    plt.ylabel(\"J\")\n",
    "    plt.figure(10)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Første testproblem: Punkter i planet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trening(images, labels, AdamOrPlain = 'plain', n=1000, h = 0.05, K = 20):\n",
    "    d = 2\n",
    "    W_k, b_k, omega, my = getRandomParam(d, K)\n",
    "    W_k = np.dot(W_k, 1/(d**2))\n",
    "    omega = np.dot(omega, 1/(d**2))\n",
    "\n",
    "    if AdamOrPlain == 'adam':\n",
    "        mW, mb, mOmega, mMy = getStartMandV(W_k,b_k,omega,my)\n",
    "        vW, vb, vOmega, vMy = getStartMandV(W_k, b_k, omega, my)\n",
    "\n",
    "    elif AdamOrPlain == 'plain':\n",
    "        tau = 0.1\n",
    "    else:\n",
    "        print(\"Må være enten adam eller plain\")\n",
    "\n",
    "    J_list = []\n",
    "\n",
    "    for i in range(n):\n",
    "        Y_k = [images]\n",
    "        for i in range(W_k.shape[0]):\n",
    "            Y_next = findNextYMat(Y_k[-1], W_k[i], b_k[i], h)\n",
    "            Y_k.append(Y_next)\n",
    "\n",
    "        z = findZvalue(Y_k[-1], omega, my)\n",
    "        dJdW, dJdb, dJdOmega, dJdMy = findGradJ(z, W_k, Y_k[:-1], Y_k[-1], omega, labels, my, b_k, h, K)\n",
    "\n",
    "        if AdamOrPlain == 'adam':\n",
    "            mW, vW, JW = adam2(mW, vW, dJdW, i + 1)\n",
    "            mb, vb, Jb = adam2(mb, vb, dJdb, i + 1)\n",
    "            mOmega, vOmega, JOmega = adam2(mOmega, vOmega, dJdOmega, i + 1)\n",
    "            mMy, vMy, JMy = adam2(mMy, vMy, dJdMy, i + 1)\n",
    "\n",
    "            W_k -= JW\n",
    "            b_k -= Jb\n",
    "            omega -= JOmega\n",
    "            my -= JMy\n",
    "\n",
    "        elif AdamOrPlain == 'plain':\n",
    "            W_k, b_k, omega, my = newU(W_k, b_k, omega, my, tau, dJdW, dJdb, dJdOmega, dJdMy)\n",
    "\n",
    "        J_list.append(J(z, labels))\n",
    "\n",
    "    return W_k, b_k, omega, my, z, J_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 PIP: Plain vanilla gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, labels = read_spiral_from_file()\n",
    "W_pip_plain, b_pip_plain, omega_pip_plain, my_pip_plain, z_pip_plain, J_pip_plain = trening(images, labels, h = 0.05, n = 1000, AdamOrPlain = 'plain')\n",
    "success_pip_plain = success(z_pip_plain,labels*1)\n",
    "\n",
    "plotJ(J_pip_plain)\n",
    "print(\"Suksessrate:\",success_pip_plain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 PIP: ADAM descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, labels = read_spiral_from_file()\n",
    "W_pip_adam, b_pip_adam, omega_pip_adam, my_pip_adam, z_pip_adam, J_pip_adam = trening(images, labels, h = 0.1, n = 2000, AdamOrPlain = 'adam')\n",
    "success_pip_adam = success(z_pip_adam,labels*1)\n",
    "\n",
    "plotJ(J_pip_adam)\n",
    "print(\"Suksessrate:\",success_pip_adam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_learned1, b_learned1, omega_learned1, my_learned1 = readParams(K = 20, d = 2, filename = 'spiral_adam_2000_it_0.1')\n",
    "print(\"Wohoo\")\n",
    "#Y_k1 = forward_function(images, W_learned1, b_learned1, h)\n",
    "#plot_progression(np.array(Y_k1), labels*1)\n",
    "#plot_separation(findZvalue, Y_k1[-1], omega_learned1, my_learned1, labels*1, 2000)\n",
    "#plot_model(forward_function, Y_k1[0], W_learned1, b_learned1, 0.1, labels*1, 100)\n",
    "#print(success1)\n",
    "#writeParams(W_learned1, b_learned1, omega_learned1, my_learned1, filename = 'spiral_adam_2000_it_0.1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Andre testproblem: Siffergjenkjenning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trening_mnist(images, labels, AdamOrPlain = 'plain', n=1000, h = 0.05, K = 20):\n",
    "    d = 28**2\n",
    "    W_k, b_k, omega, my = getRandomParam(d, K)\n",
    "    W_k = np.dot(W_k, 1/(d**2))\n",
    "    omega = np.dot(omega, 1/(d**2))\n",
    "\n",
    "    if AdamOrPlain == 'adam':\n",
    "        mW, mb, mOmega, mMy = getStartMandV(W_k,b_k,omega,my)\n",
    "        vW, vb, vOmega, vMy = getStartMandV(W_k, b_k, omega, my)\n",
    "\n",
    "    elif AdamOrPlain == 'plain':\n",
    "        tau = 0.1\n",
    "    else:\n",
    "        print(\"Må være enten adam eller plain\")\n",
    "\n",
    "    J_list = []\n",
    "\n",
    "    for i in range(n):\n",
    "        chunk_start = random.randint(0, images.shape[1] - 500)\n",
    "        Y_k = [images[:, chunk_start:chunk_start+500]]\n",
    "        J_list.append(J(findZvalue(Y_k[-1], omega, my), labels[chunk_start:chunk_start+500]))\n",
    "        for i in range(W_k.shape[0]):\n",
    "            Y_next = findNextYMat(Y_k[-1], W_k[i], b_k[i], h)\n",
    "            Y_k.append(Y_next)\n",
    "        print(\"Ferdig med matrisene.\")\n",
    "\n",
    "        z = findZvalue(Y_k[-1], omega, my)\n",
    "        dJdW, dJdb, dJdOmega, dJdMy = findGradJ(z, W_k, Y_k[:-1], Y_k[-1], omega, labels[chunk_start:chunk_start+500], my, b_k, h, K)\n",
    "\n",
    "        if AdamOrPlain == 'adam':\n",
    "            mW, vW, JW = adam2(mW, vW, dJdW, i + 1)\n",
    "            mb, vb, Jb = adam2(mb, vb, dJdb, i + 1)\n",
    "            mOmega, vOmega, JOmega = adam2(mOmega, vOmega, dJdOmega, i + 1)\n",
    "            mMy, vMy, JMy = adam2(mMy, vMy, dJdMy, i + 1)\n",
    "\n",
    "            W_k -= JW\n",
    "            b_k -= Jb\n",
    "            omega -= JOmega\n",
    "            my -= JMy\n",
    "\n",
    "        elif AdamOrPlain == 'plain':\n",
    "            W_k, b_k, omega, my = newU(W_k, b_k, omega, my, tau, dJdW, dJdb, dJdOmega, dJdMy)\n",
    "\n",
    "    J_list.append(J(z, labels[chunk_start:chunk_start+500]))\n",
    "\n",
    "    return W_k, b_k, omega, my, z, J_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, labels_raw = get_dataset(dataset = 'training')\n",
    "labels = np.array([label[0]*1 for label in labels_raw])\n",
    "\n",
    "W_mnist, b_mnist, omega_mnist, my_mnist, z_mnist, J_list_mnist = trening_mnist(images, labels, AdamOrPlain = 'adam', n = 5, h = 0.05, K = 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotJ(J_list_mnist)\n",
    "Y_mnist = forward_function(images, W_mnist, b_mnist, h = 0.05)\n",
    "z_mnist = findZvalue(Y_mnist[-1], omega_minst, my_mnist)\n",
    "success_mnist = success(z_mnist, labels)\n",
    "print(success_mnist)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
